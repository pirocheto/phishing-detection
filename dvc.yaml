# This DVC pipeline defines a sequence of stages for a machine learning workflow.
# The workflow involves downloading, preprocessing, feature selection, training,
# testing, and interpreting a model. It utilizes Data Version Control (DVC) to
# manage and version the data and code dependencies.

# Each stage specifies the commands ('cmd'), parameters, dependencies, outputs,
# and optional plots associated with the corresponding task.

# To understand the role of each parameter see:
# https://dvc.org/doc/user-guide/project-structure/dvcyaml-files

stages:
  # Stage 0: Download Data
  # - Downloads data from a specified source using the 'wget' command.
  download_data:
    cmd:
      - wget -q ${data_url} -O ${path.data.all}
    params:
      - data_url
      - path.data.all
    deps:
      - ${data_url}
    outs:
      - ${path.data.all}:
          cache: False

  # Stage 1: Analyze Data
  # - Analyze data with feature correlations and statistics plots
  analyze_data:
    cmd:
      - python src/analyze_data.py
    params:
      - column_mapping
      - path.data.all
      - path.results.plots.correlation_matrix
      - path.results.plots.feature_statistics
      - plt_style
      - feature_dtypes
    deps:
      - ${path.data.all}
      - src/analyze_data.py
    plots:
      - ${path.results.plots.correlation_matrix}:
          cache: False
      - ${path.results.plots.feature_statistics}:
          cache: False

  # Stage 2: Split Data
  # - Splits the downloaded data into training and testing sets.
  # - Generates a plot to visualize the proportion of data in each split.
  split_data:
    cmd: python src/split_data.py
    params:
      - train_test_split
      - column_mapping
      - path.data.all
      - path.data.raw
      - path.results.plots.data_proportion
      - plt_style
    deps:
      - src/split_data.py
      - ${path.data.all}
    plots:
      - ${path.results.plots.data_proportion}:
          cache: False
    outs:
      - ${path.data.raw.dir}:
          cache: False

  # Stage 3: Feature Selection
  # - Removes the least important features to accelerate training and improve model interpretability.
  feature_selection:
    cmd: python src/feature_selection.py
    params:
      - column_mapping
      - feature_selection
      - path.data.raw
      - path.data.selected
      - path.results.selected_features
      - plt_style
    deps:
      - ${path.data.raw.dir}
      - src/feature_selection.py
    outs:
      - ${path.data.selected.dir}:
          cache: False

  # Stage 4: Preprocessing
  # - Applies preprocessing steps to the training and testing data.
  # - Includes tasks like label encoding, feature normalization, and/or standardization.
  preprocessing:
    cmd: python src/preprocessing.py
    params:
      - preprocessing
      - path.data.selected
      - path.data.transformed
      - column_mapping
    deps:
      - src/preprocessing.py
      - ${path.data.selected.dir}
    outs:
      - ${path.data.transformed.dir}:
          cache: False

  # Stage 5: Train Model
  # - Trains a machine learning model using the selected features from the previous stage.
  # - Saves the trained model in binary format.
  train_model:
    cmd: python src/train_model.py
    params:
      - column_mapping
      - pipeline
      - path.data.transformed
      - path.results.model_bin
    deps:
      - src/train_model.py
      - ${path.data.transformed.dir}
    outs:
      - ${path.results.model_bin}:
          cache: False

  # Stage 6: Test Model
  # - Tests the trained model on the testing data and evaluates its performance.
  # - Creates multiple plots, including confusion matrix, ROC curve, and precision-recall curve.
  test_model:
    cmd: python src/test_model.py
    params:
      - column_mapping
      - plt_style
      - path.data.transformed
      - path.data.predicted
      - path.results.model_bin
      - path.results.metrics
      - path.results.classification_report
      - path.results.plots.confusion_matrix
      - path.results.plots.roc_curve
      - path.results.plots.score_distribution
      - path.results.plots.precision_recall_curve
      - path.results.plots.calibration_curve
      - path.results.plots.cumulative_distribution
      - path.results.plots.metrics_table
    deps:
      - src/test_model.py
      - ${path.data.transformed.dir}
      - ${path.results.model_bin}
    plots:
      - ${path.results.plots.confusion_matrix}:
          cache: False
      - ${path.results.plots.roc_curve}:
          cache: False
      - ${path.results.plots.score_distribution}:
          cache: False
      - ${path.results.plots.precision_recall_curve}:
          cache: False
      - ${path.results.plots.cumulative_distribution}:
          cache: False
      - ${path.results.plots.calibration_curve}:
          cache: False
      - ${path.results.plots.metrics_table}:
          cache: False
    outs:
      - ${path.data.predicted.dir}:
          cache: False
      - ${path.results.classification_report}:
          cache: False

    metrics:
      - ${path.results.metrics}:
          cache: False

  # Stage 7: Model Interpretation
  # - Provides interpretation of the trained model by creating plots such as feature importances.
  model_interpretation:
    cmd:
      - python src/model_interpretation.py
    params:
      - path.data.transformed
      - column_mapping
      - path.results.model_bin
      - path.results.plots.feature_importances
      - plt_style
    deps:
      - src/model_interpretation.py
      - ${path.data.transformed.dir}
      - ${path.results.model_bin}
    plots:
      - ${path.results.plots.feature_importances}:
          cache: False

  # TODO: Build Pipeline with fitted estimators and save it in ONNX Format
